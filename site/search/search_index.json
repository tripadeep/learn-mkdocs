{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"PySpark Documentation","text":"<p>Welcome to the PySpark Documentation \u2014 a practical, concise reference for working with PySpark. This site covers core concepts, common APIs, tutorials, and reference material for deploying and configuring PySpark applications.</p>"},{"location":"#quick-links","title":"Quick links","text":"<ul> <li>Getting Started \u2014 guides/getting-started.md</li> <li>Core Concepts \u2014 guides/core-concepts.md</li> <li>DataFrame API \u2014 guides/dataframe-api.md</li> <li>Tutorials \u2014 tutorials/wordcount.md, tutorials/etl-pipeline.md</li> <li>Reference \u2014 reference/configuration.md</li> </ul> <p>This documentation is written for PySpark 3.x and assumes a basic familiarity with Python. Code examples use the PySpark Python API and show common patterns for local development and cluster deployment.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome improvements. To propose changes, edit the repository and open a pull request. See <code>CONTRIBUTING.md</code> for guidelines.</p>"},{"location":"tags/","title":"Tags","text":"<p>This page lists tags used across the documentation and blog posts.</p>"},{"location":"blog/","title":"PySpark Blog","text":"<p>Short updates, tutorials, and tips about using PySpark for data processing.</p> <ul> <li>Tutorials: Word Count, ETL Pipeline</li> <li>Tags: <code>pyspark</code>, <code>dataframe</code>, <code>streaming</code>, <code>spark-submit</code></li> </ul> <p>Stay tuned for concise how-tos and examples focused on practical PySpark usage.</p>"},{"location":"blog/tags/","title":"Blog Tags","text":"<p>This page lists tags used in the PySpark blog.</p>"},{"location":"blog/author/team/","title":"PySpark Docs Team","text":"<p>A group of engineers and data practitioners maintaining these PySpark guides and tutorials. Contributors focus on practical examples, reproducible pipelines, and clear API usage.</p> <p>If you'd like to contribute, add your profile to <code>docs/blog/.authors.yml</code> and open a pull request.</p>"},{"location":"blog/posts/draft/","title":"PySpark Tips: Efficient Filtering","text":"<p>Quick tip: prefer <code>filter</code> on DataFrames with column expressions rather than Python functions for better performance because Spark can optimize the filter operation.</p> <pre><code>from pyspark.sql.functions import col\n\ndf_filtered = df.filter(col(\"value\") &gt; 100)\n</code></pre> <p>This is a draft post \u2014 polish and publish when ready.</p>","tags":["pyspark","tips"]},{"location":"blog/posts/myfirst/","title":"Introduction to PySpark","text":"<p>PySpark is the Python API for Apache Spark \u2014 a unified analytics engine for large-scale data processing. This post explains what PySpark is, when to use it, and how to start a local Spark session.</p>","tags":["pyspark","getting-started"]},{"location":"blog/posts/myfirst/#quick-example","title":"Quick example","text":"<pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \n    .appName(\"pyspark-intro\") \n    .master(\"local[*]\") \n    .getOrCreate()\n\nprint(spark.version)\n\nspark.stop()\n</code></pre> <p>For hands-on examples, see the Tutorials section of this site.</p>","tags":["pyspark","getting-started"]},{"location":"blog/posts/drafts/may_4th/","title":"Draft: Partitioning Strategies in PySpark","text":"<p>Notes on partitioning for performance and shuffles. Topics to finish: examples for <code>repartition</code>, <code>coalesce</code>, and partitioning by column on write.</p> <ul> <li>When to repartition before a shuffle</li> <li>When to use <code>coalesce</code> to reduce partitions</li> <li>Partitioning writes by columns for faster reads</li> </ul> <p>(Work in progress.)</p>","tags":["pyspark","partitioning"]},{"location":"blog/posts/drafts/may_day/","title":"Draft: Handling Nulls and Defaults","text":"<p>Draft notes on handling nulls in DataFrames, using <code>na.fill</code>, <code>na.drop</code>, and safe SQL expressions.</p> <p>Examples to add:</p> <pre><code>df.na.fill({\"col1\": 0, \"col2\": \"unknown\"})\n</code></pre> <p>(Work in progress.)</p>","tags":["pyspark","nulls"]},{"location":"blog/posts/drafts/pirate_day/","title":"Draft: When to Use UDFs vs Built-in Functions","text":"<p>Notes comparing performance and use-cases of Python UDFs and built-in Spark SQL functions. To complete: microbenchmarks and using pandas UDFs.</p> <p>(Work in progress.)</p>","tags":["pyspark","udfs"]},{"location":"blog/posts/drafts/sysadmin_day/","title":"Draft: Monitoring PySpark Jobs","text":"<p>Notes on monitoring, including the Spark UI, metrics, and useful logging configurations. To complete: examples for enabling metrics and connecting to Prometheus.</p> <p>(Work in progress.)</p>","tags":["pyspark","monitoring"]},{"location":"blog/posts/drafts/ten_years/","title":"Roadmap: PySpark Docs","text":"<p>Planned topics for this documentation site: core API, tutorials, deployment guides, and reference pages. Contributions welcome.</p> <p>(Work in progress.)</p>","tags":["pyspark","roadmap"]},{"location":"guides/core-concepts/","title":"Core Concepts","text":"<p>This page explains PySpark concepts: RDDs, DataFrames, SparkSession, transformations vs actions, and partitioning.</p>"},{"location":"guides/core-concepts/#rdd-vs-dataframe","title":"RDD vs DataFrame","text":"<ul> <li>RDD: low-level resilient distributed dataset.</li> <li>DataFrame: higher-level table-like API with optimizations via Catalyst.</li> </ul>"},{"location":"guides/core-concepts/#transformations-and-actions","title":"Transformations and Actions","text":"<ul> <li>Transformations are lazy (e.g., map, filter, select).</li> <li>Actions trigger execution (e.g., show, collect, write).</li> </ul>"},{"location":"guides/core-concepts/#partitioning","title":"Partitioning","text":"<ul> <li>Data is divided into partitions for parallelism.</li> <li>Repartition and coalesce can change partition counts.</li> </ul>"},{"location":"guides/core-concepts/#lazy-evaluation-and-optimization","title":"Lazy evaluation and optimization","text":"<ul> <li>Spark builds a logical plan and optimizes it before execution.</li> </ul>"},{"location":"guides/dataframe-api/","title":"DataFrame API","text":"<p>This page covers common DataFrame operations: read, select, filter, groupBy, joins, and write.</p>"},{"location":"guides/dataframe-api/#reading-data","title":"Reading data","text":"<pre><code># CSV\ndf = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n\n# Parquet\ndf = spark.read.parquet(\"path/to/parquet\")\n</code></pre>"},{"location":"guides/dataframe-api/#selecting-and-filtering","title":"Selecting and filtering","text":"<pre><code>from pyspark.sql.functions import col\n\ndf2 = df.select(\"id\", \"name\").filter(col(\"id\") &gt; 10)\ndf2.show()\n</code></pre>"},{"location":"guides/dataframe-api/#grouping","title":"Grouping","text":"<pre><code>df.groupBy(\"category\").count().show()\n</code></pre>"},{"location":"guides/dataframe-api/#joins","title":"Joins","text":"<pre><code>joined = df.alias(\"a\").join(other.alias(\"b\"), on=[\"id\"], how=\"inner\")\n</code></pre>"},{"location":"guides/dataframe-api/#writing-data","title":"Writing data","text":"<pre><code>df.write.mode(\"overwrite\").parquet(\"output/path\")\n</code></pre>"},{"location":"guides/getting-started/","title":"Getting Started with PySpark","text":"<p>This guide shows how to install PySpark, create a SparkSession, and run simple examples locally.</p>"},{"location":"guides/getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+ (recommended)</li> <li>Java JDK 8/11 installed</li> <li>pip</li> </ul>"},{"location":"guides/getting-started/#install-pyspark","title":"Install PySpark","text":"<pre><code>python -m pip install pyspark\n</code></pre>"},{"location":"guides/getting-started/#create-a-sparksession","title":"Create a SparkSession","text":"<pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \n    .appName(\"local-example\") \n    .master(\"local[*]\") \n    .getOrCreate()\n\n# show version\nprint(spark.version)\n</code></pre>"},{"location":"guides/getting-started/#create-a-dataframe","title":"Create a DataFrame","text":"<pre><code>data = [(1, \"Alice\"), (2, \"Bob\")]\ncolumns = [\"id\", \"name\"]\ndf = spark.createDataFrame(data, columns)\ndf.show()\n</code></pre>"},{"location":"guides/getting-started/#stop-the-session","title":"Stop the session","text":"<pre><code>spark.stop()\n</code></pre>"},{"location":"guides/rdd-api/","title":"RDD API","text":"<p>Use RDDs for fine-grained control and when you need custom partitioning or lower-level APIs.</p> <pre><code>rdd = spark.sparkContext.parallelize([1,2,3,4])\nmapped = rdd.map(lambda x: x * 2)\nprint(mapped.collect())\n</code></pre>"},{"location":"guides/sql-sparksession/","title":"SQL &amp; SparkSession","text":"<p>SparkSession is the entry point for DataFrame and SQL functionality.</p> <pre><code>spark.sql(\"SELECT 1\").show()\n</code></pre>"},{"location":"guides/sql-sparksession/#registering-temporary-views","title":"Registering temporary views","text":"<pre><code>df.createOrReplaceTempView(\"my_view\")\nres = spark.sql(\"SELECT * FROM my_view WHERE id &gt; 10\")\nres.show()\n</code></pre>"},{"location":"guides/structured-streaming/","title":"Structured Streaming","text":"<p>Structured Streaming is Spark's high-level streaming API. It provides a table-like interface over streaming data.</p> <pre><code>from pyspark.sql.functions import expr\n\nstream = spark.readStream.format(\"socket\").option(\"host\",\"localhost\").option(\"port\",9999).load()\ncounts = stream.groupBy(\"value\").count()\nquery = counts.writeStream.outputMode(\"complete\").format(\"console\").start()\nquery.awaitTermination()\n</code></pre>"},{"location":"reference/configuration/","title":"Configuration","text":"<p>Common Spark configuration options and how to set them with SparkConf or SparkSession builder.</p> <pre><code>from pyspark import SparkConf\nfrom pyspark.sql import SparkSession\n\nconf = SparkConf().set(\"spark.app.name\", \"app\").set(\"spark.executor.memory\", \"2g\")\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\n</code></pre>"},{"location":"reference/configuration/#common-configs","title":"Common configs","text":"<ul> <li><code>spark.executor.memory</code> \u2014 memory per executor</li> <li><code>spark.executor.cores</code> \u2014 cores per executor</li> <li><code>spark.sql.shuffle.partitions</code> \u2014 default shuffle partitions</li> </ul>"},{"location":"reference/deployment/","title":"Deployment","text":"<p>Best practices for packaging and submitting PySpark jobs.</p>"},{"location":"reference/deployment/#submit-with-spark-submit","title":"Submit with spark-submit","text":"<pre><code>spark-submit --master yarn --deploy-mode cluster \\\n  --conf spark.executor.memory=4g \\\n  --py-files dependencies.zip \\\n  app.py\n</code></pre>"},{"location":"reference/deployment/#packaging","title":"Packaging","text":"<ul> <li>Use <code>spark-submit</code> with <code>--py-files</code> or build an egg/wheel.</li> <li>Ensure dependencies are available on the cluster or packaged with the job.</li> </ul>"},{"location":"tutorials/etl-pipeline/","title":"ETL Pipeline (PySpark tutorial)","text":"<p>This tutorial demonstrates a simple ETL pipeline: read CSV, transform, and write Parquet.</p> <pre><code>from pyspark.sql.functions import col, to_date\n\ndf = spark.read.csv(\"data/input.csv\", header=True, inferSchema=True)\ndf2 = df.withColumn(\"date\", to_date(col(\"date_str\"), \"yyyy-MM-dd\"))\ndf2.filter(col(\"amount\") &gt; 0).write.mode(\"overwrite\").parquet(\"data/output\")\n</code></pre>"},{"location":"tutorials/wordcount/","title":"Word Count (PySpark tutorial)","text":"<p>A minimal word count example using RDDs and DataFrames.</p>"},{"location":"tutorials/wordcount/#rdd-version","title":"RDD version","text":"<pre><code>rdd = spark.sparkContext.textFile(\"data/text.txt\")\ncounts = rdd.flatMap(lambda line: line.split()) \n            .map(lambda w: (w,1)) \n            .reduceByKey(lambda a,b: a+b)\ncounts.collect()\n</code></pre>"},{"location":"tutorials/wordcount/#dataframe-version","title":"DataFrame version","text":"<pre><code>from pyspark.sql.functions import explode, split, col\n\ntext = spark.read.text(\"data/text.txt\")\nwords = text.select(explode(split(col(\"value\"), \"\\s+\")).alias(\"word\"))\ncounts = words.groupBy(\"word\").count()\ncounts.show()\n</code></pre>"}]}